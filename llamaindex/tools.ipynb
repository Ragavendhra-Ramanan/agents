{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function calling tool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='The weather in New York is sunny with low temperature.', tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny with low temperature.', is_error=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def get_weather(location: str) ->str:\n",
    "    \"\"\"\n",
    "    Useful for getting the weather for a given location.\n",
    "    \"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny with low temperature.\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(get_weather,\n",
    "name= \"my_weather_tool\",\n",
    "description=\"Useful for getting weather for a given location\")\n",
    "\n",
    "tool.call(\"New York\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The name and description default to the name and docstring from the provided function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When using an agent or LLM with function calling, the tool selected (and the arguments written for that tool) rely strongly on the tool name and description of the purpose and arguments of the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Query Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "embed_model = HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "tool = QueryEngineTool.from_defaults(query_engine,name='answer based on persona',description=\"alfred_chroma_db documents to answer questions based on persona\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tool specs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(ToolMetadata(description=\"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\", name='load_data', fn_schema=<class 'llama_index.core.tools.utils.load_data'>, return_direct=False),\n",
       "  \"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\"),\n",
       " (ToolMetadata(description=\"search_messages(query: str, max_results: Optional[int] = None)\\nSearches email messages given a query string and the maximum number\\n        of results requested by the user\\n           Returns: List of relevant message objects up to the maximum number of results.\\n\\n        Args:\\n            query[str]: The user's query\\n            max_results (Optional[int]): The maximum number of search results\\n            to return.\\n        \", name='search_messages', fn_schema=<class 'llama_index.core.tools.utils.search_messages'>, return_direct=False),\n",
       "  \"search_messages(query: str, max_results: Optional[int] = None)\\nSearches email messages given a query string and the maximum number\\n        of results requested by the user\\n           Returns: List of relevant message objects up to the maximum number of results.\\n\\n        Args:\\n            query[str]: The user's query\\n            max_results (Optional[int]): The maximum number of search results\\n            to return.\\n        \"),\n",
       " (ToolMetadata(description=\"create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\\nCreate and insert a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n        \", name='create_draft', fn_schema=<class 'llama_index.core.tools.utils.create_draft'>, return_direct=False),\n",
       "  \"create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\\nCreate and insert a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n        \"),\n",
       " (ToolMetadata(description=\"update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\\nUpdate a draft email.\\n           Print the returned draft's message and id.\\n           This function is required to be passed a draft_id that is obtained when creating messages\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n            draft_id (str): the id of the draft to be updated\\n        \", name='update_draft', fn_schema=<class 'llama_index.core.tools.utils.update_draft'>, return_direct=False),\n",
       "  \"update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\\nUpdate a draft email.\\n           Print the returned draft's message and id.\\n           This function is required to be passed a draft_id that is obtained when creating messages\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n            draft_id (str): the id of the draft to be updated\\n        \"),\n",
       " (ToolMetadata(description=\"get_draft(draft_id: str = None) -> str\\nGet a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n        \", name='get_draft', fn_schema=<class 'llama_index.core.tools.utils.get_draft'>, return_direct=False),\n",
       "  \"get_draft(draft_id: str = None) -> str\\nGet a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n        \"),\n",
       " (ToolMetadata(description=\"send_draft(draft_id: str = None) -> str\\nSends a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n        \", name='send_draft', fn_schema=<class 'llama_index.core.tools.utils.send_draft'>, return_direct=False),\n",
       "  \"send_draft(draft_id: str = None) -> str\\nSends a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n        \")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "[(tool.metadata,tool.metadata.description) for tool in tool_spec_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility tools**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. OnDemandToolLoader - turns Llamaindex data loader into a tool that an agent can use.load data,index and query ... on demand\n",
    "2. Load and search spec tool - loading tool and search tool .. loading index it search retrives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
